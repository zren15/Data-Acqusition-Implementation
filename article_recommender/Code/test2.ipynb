{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T03:02:32.877125Z",
     "start_time": "2021-10-02T03:02:09.767915Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mydict={}\n",
    "with open('/Users/Renne/data/glove.6B.300d.txt','r', encoding=\"utf-8\") as f:\n",
    "    for lines in f:\n",
    "        split = lines.split()\n",
    "        word = split[0]\n",
    "        vector = np.asarray(split[1:], \"float32\")\n",
    "        mydict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-02T03:02:54.992Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def filelist(root):\n",
    "    \"\"\"Return a fully-qualified list of filenames under root directory\"\"\"\n",
    "    allfiles = []\n",
    "    for path, subdirs, files in os.walk(root):\n",
    "        for name in files:\n",
    "            allfiles.append(os.path.join(path, name))\n",
    "    return allfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-02T02:56:17.720Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_text(filename):\n",
    "    \"\"\"\n",
    "    Load and return the text of a text file, assuming latin-1 encoding as that\n",
    "    is what the BBC corpus uses.  Use codecs.open() function not open().\n",
    "    \"\"\"\n",
    "    f = codecs.open(filename, encoding='latin-1', mode='r')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-02T02:56:19.584Z"
    }
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "files = filelist('/Users/Renne/data/bbc/')\n",
    "filter_file=[]\n",
    "for i in range(len(files)):\n",
    "    if files[i].endswith('.txt') == True:\n",
    "        filter_file.append(files[i])\n",
    "\n",
    "print(filter_file[0])\n",
    "title=[]\n",
    "a_mins_t=[]\n",
    "for j in range(len(filter_file)):\n",
    "    t = get_text(filter_file[j]).split('\\n')[0]\n",
    "    cont = get_text(filter_file[j]).split('\\n')[1:]\n",
    "    cont = ''.join(cont)\n",
    "    title.append(t)\n",
    "    a_mins_t.append(cont)\n",
    "print(title[1])\n",
    "print(a_mins_t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T02:31:55.026951Z",
     "start_time": "2021-10-02T02:31:55.023480Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_glove(filename):\n",
    "    \"\"\"\n",
    "    Read all lines from the indicated file and return a dictionary\n",
    "    mapping word:vector where vectors are of numpy `array` type.\n",
    "    GloVe file lines are of the form:\n",
    "\n",
    "    the 0.418 0.24968 -0.41242 0.1217 ...\n",
    "\n",
    "    So split each line on spaces into a list; the first element is the word\n",
    "    and the remaining elements represent factor components. The length of the vector\n",
    "    should not matter; read vectors of any length.\n",
    "\n",
    "    When computing the vector for each document, use just the text, not the text and title.\n",
    "    \"\"\"\n",
    "    mydict={}\n",
    "    for lines in filename:\n",
    "        split = line.split()\n",
    "        word = split[0]\n",
    "        vector = np.asarray(split[1:], \"float32\")\n",
    "        mydict[word] = vector\n",
    "    return mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-02T02:46:26.522Z"
    }
   },
   "outputs": [],
   "source": [
    "def doc2vec(text, gloves):\n",
    "    \"\"\"\n",
    "    Return the word vector centroid for the text. Sum the word vectors\n",
    "    for each word and then divide by the number of words. Ignore words\n",
    "    not in gloves.\n",
    "    \"\"\"\n",
    "    text = words(text)\n",
    "    total_wordvec = 0\n",
    "    total_count = 0\n",
    "    for i in range(len(text)):\n",
    "        if text[i] in gloves.keys():\n",
    "            total_count += 1\n",
    "            total_wordvec += glove[text[i]]\n",
    "    doc2vec = total_wordvec/total_count\n",
    "    return doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_articles(articles_dirname, gloves):\n",
    "    \"\"\"\n",
    "    Load all .txt files under articles_dirname and return a table (list of lists/tuples)\n",
    "    where each record is a list of:\n",
    "\n",
    "      [filename, title, article-text-minus-title, wordvec-centroid-for-article-text]\n",
    "\n",
    "    We use gloves parameter to compute the word vectors and centroid.\n",
    "\n",
    "    The filename is fully-qualified name of the text file including\n",
    "    the path to the root of the corpus passed in on the command line.\n",
    "    \"\"\"\n",
    "    files = filelist(articles_dirname)\n",
    "    filter_file=[]\n",
    "    for i in range(len(files)):\n",
    "        if files[i].endswith('.txt') == True:\n",
    "            filter_file.append(files[i])\n",
    "    for j in range(len(filter_file)):\n",
    "        t = get_text(filter_file[j]).split('\\n')[0]\n",
    "        cont = get_text(filter_file[j]).split('\\n')[1:]\n",
    "        cont = ''.join(cont)\n",
    "        wordvec = doc2vec(cont,gloves)\n",
    "        table.append([filter_file[j],t,cont,wordvec])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T03:01:09.725415Z",
     "start_time": "2021-10-03T03:01:09.722862Z"
    }
   },
   "outputs": [],
   "source": [
    "filename= '/Users/Renne/data/bbc/entertainment/289.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T00:02:20.286962Z",
     "start_time": "2021-10-03T00:02:20.283758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entertainment/289.txt\n"
     ]
    }
   ],
   "source": [
    "f = filename.split(\"/\")[-2:]\n",
    "f = '/'.join(f)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T03:01:24.305845Z",
     "start_time": "2021-10-03T03:01:24.302637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/article/entertainment/289.txt\n"
     ]
    }
   ],
   "source": [
    "path = filename.split(\"/\")[-2:]\n",
    "path = '/'.join(path)\n",
    "print(f'/article/{path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
